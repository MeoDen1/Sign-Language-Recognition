{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prepare environment\n",
    "# %cd ../input/sign-language-components-1/Kaggle-folder\n",
    "# !pip install keras_preprocessing\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import random\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "from tensorflow import data, GradientTape\n",
    "from keras import layers, models\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "from keras.optimizers import Adam\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from dataset import Dataset, pre_processing_1\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Kaggle \n",
    "dataset_path = \"Dataset/phoenix14t.pami0.dev.annotations_only/phoenix14t.pami0.dev.dontsync.annotations_only\"\n",
    "class Dataset:\n",
    "    def __init__(self) -> None:\n",
    "        with open(dataset_path, 'rb') as f:\n",
    "            # annotations: list of object\n",
    "            # - name: path (train/...)\n",
    "            # - signer: signer name\n",
    "            # - gloss: JETZT ...\n",
    "            # - text: ...\n",
    "            self.annotations = pickle.load(f)\n",
    "\n",
    "    def load(self, path=\"Dataset/videos_phoenix/videos\", size: int=10) -> list:\n",
    "        \"\"\"\n",
    "        Return format: List of object: {cap, gloss, text}\n",
    "        \"\"\"\n",
    "        # shuffle all annotations\n",
    "        random.shuffle(self.annotations)\n",
    "        count = 0\n",
    "        data = []\n",
    "\n",
    "        for obj in self.annotations:\n",
    "            if count >= size:\n",
    "                break\n",
    "        \n",
    "            vid_path = os.path.join(path, obj[\"name\"]) + \".mp4\"\n",
    "            cap = cv2.VideoCapture(vid_path)\n",
    "            ret = True\n",
    "            frames = []\n",
    "\n",
    "            while ret:\n",
    "                ret, img = cap.read()\n",
    "                if ret:\n",
    "                    frames.append(img)\n",
    "\n",
    "            # Check if the video exists\n",
    "            if len(frames) == 0:\n",
    "                continue\n",
    "\n",
    "            frames = np.array(frames)\n",
    "\n",
    "            count += 1\n",
    "            data.append({'path': vid_path, 'frames': frames, 'gloss': obj[\"gloss\"], \"text\": obj[\"text\"]})\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GLOBAL VARIABLE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "heads = 4\n",
    "d_k = 64\n",
    "d_v = 64\n",
    "d_ff = 64\n",
    "encoder_seq_len = 200\n",
    "decoder_seq_len = 20\n",
    "d_model = 128\n",
    "rate=0.2\n",
    "\n",
    "epochs = 2\n",
    "batch_size = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = Dataset()\n",
    "sample_data = dataset.load(size=20)\n",
    "tokenizer, sample_encoder_input, sample_decoder_input, _ = pre_processing_1(sample_data, encoder_seq_len, decoder_seq_len)\n",
    "vocab_size = len(tokenizer.word_index)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(sample_encoder_input, sample_decoder_input, test_size=0.2)\n",
    "\n",
    "train_dataset = data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "\n",
    "# Define model\n",
    "training_model = TransformerModel(encoder_seq_len, vocab_size, decoder_seq_len, heads, d_k, d_v, d_ff, d_model, rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialEmbedding(layers.Layer):\n",
    "    \"\"\"\n",
    "    Convert sequence of frames into sequence of vectors\n",
    "\n",
    "    Parameters\n",
    "    --\n",
    "    x : array_like\n",
    "        Array with shape (batch_size, seq_len, height, width, channels)\n",
    "\n",
    "    Return\n",
    "    --\n",
    "    output : ndarray\n",
    "        Tensor with shape (batch_size, seq_len, embedding_dim)\n",
    "    \"\"\"\n",
    "    def __init__(self, seq_len, embedding_dim: int = 64, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.conv_2d_1 = layers.Conv2D(16, 3, activation='relu')\n",
    "        self.max_pooling_1 = layers.MaxPooling2D()\n",
    "        self.conv_2d_2 = layers.Conv2D(32, 3, activation='relu')\n",
    "        self.W_1 = layers.Dense(embedding_dim)\n",
    "        self.flatten = layers.Flatten()\n",
    "\n",
    "        # position encoding\n",
    "        self.positional_encoding = self.get_positional_encoding(seq_len, embedding_dim)\n",
    "\n",
    "    def get_positional_encoding(self, seq_len, d, n=10000):\n",
    "        \n",
    "        P = np.zeros((seq_len, d))\n",
    "        for k in range(seq_len):\n",
    "            for i in range(int(d / 2)):\n",
    "                denominator = pow(n, 2 * i / d)\n",
    "                P[k, 2 * i] = np.sin(k / denominator)\n",
    "                P[k, 2 * i + 1] = np.cos(k / denominator)\n",
    "        \n",
    "        return P\n",
    "\n",
    "    def call(self, x):\n",
    "        # x: (batch_size, seq_len, height, width, channels)\n",
    "        processed_frames = []\n",
    "\n",
    "        for i in range(self.seq_len):\n",
    "            frame = x[:, i, :, :, :]\n",
    "            # Frame shape: (batch_size, height, width, channels)\n",
    "\n",
    "            # Applying Conv2D\n",
    "            x1 = self.conv_2d_1(frame)\n",
    "            x1 = self.max_pooling_1(x1)\n",
    "            x1 = self.conv_2d_2(x1)\n",
    "\n",
    "            # Flatten and set final dim = embedding_dim\n",
    "            x1 = self.flatten(x1)\n",
    "            x1 = self.W_1(x1)\n",
    "\n",
    "            processed_frames.append(x1)\n",
    "\n",
    "        output = tf.stack(processed_frames, axis=1)\n",
    "        # Output shape: (batch_size, seq_len, embedding_dim)\n",
    "        output += self.positional_encoding\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbedding(layers.Layer):\n",
    "    \"\"\"\n",
    "    Convert sequence of tokens into sequence of vector\n",
    "\n",
    "    Parameters\n",
    "    --\n",
    "    x : array_like\n",
    "        List of token. Shape : (batch_size, seq_len)\n",
    "\n",
    "    Return\n",
    "    --\n",
    "    output : shape (batch_size, seq_len, embedding_dim)\n",
    "    \"\"\"\n",
    "    def __init__(self, seq_len, vocab_size, embedding_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embedding = layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.positional_encoding = self.get_positional_encoding(seq_len, embedding_dim)\n",
    "\n",
    "    def get_positional_encoding(self, seq_len, d, n=10000):\n",
    "        P = np.zeros((seq_len, d))\n",
    "        for k in range(seq_len):\n",
    "            for i in range(int(d / 2)):\n",
    "                denominator = pow(n, 2 * i / d)\n",
    "                P[k, 2 * i] = np.sin(k / denominator)\n",
    "                P[k, 2 * i + 1] = np.cos(k / denominator)\n",
    "        \n",
    "        return P\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x += self.positional_encoding\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dot Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProductAttention(layers.Layer):\n",
    "    \"\"\"\n",
    "    Dot-product Attention layer base on `Attention is all you need`\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, queries, keys, values, d_k : int, mask=None):\n",
    "        \"\"\"\n",
    "        Calculate Dot-product Attention\n",
    "        \n",
    "        Parameters\n",
    "        -\n",
    "        queries : array_like\n",
    "        keys : array_like\n",
    "        values : array_like\n",
    "        d_k: int\n",
    "        mask: array_like | None\n",
    "        \"\"\"\n",
    "        \n",
    "        scores = tf.matmul(queries, keys, transpose_b=True) / np.sqrt(d_k)\n",
    "\n",
    "        if mask is not None:\n",
    "            scores += -1e9 * mask\n",
    "\n",
    "        weights = tf.math.softmax(scores)\n",
    "\n",
    "        return tf.matmul(weights, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(layers.Layer):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention layers\n",
    "\n",
    "    Parameters\n",
    "    -\n",
    "    heads : int\n",
    "        Number of heads. `d_k` must be divisible by `heads`\n",
    "    d_k : int\n",
    "    d_v : int\n",
    "    d_model : int\n",
    "        Final layer unit value\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, heads, d_k, d_v, d_model, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.heads = heads\n",
    "        self.d_k = d_k\n",
    "        self.attention = DotProductAttention()\n",
    "        self.W_q = layers.Dense(d_k)\n",
    "        self.W_k = layers.Dense(d_k)\n",
    "        self.W_v = layers.Dense(d_v)\n",
    "        self.W_o = layers.Dense(d_model)\n",
    "\n",
    "    def reshape_tensor(self, x, heads, flag):\n",
    "        \"\"\"\n",
    "        Convert tensor shape into multi-head array and reverse\n",
    "        \"\"\"\n",
    "        if flag:\n",
    "            x = tf.reshape(x, shape=(x.shape[0], x.shape[1], heads, -1))\n",
    "            x = tf.transpose(x, perm=(0, 2, 1, 3))\n",
    "        else:\n",
    "            x = tf.transpose(x, perm=(0, 2, 1, 3))\n",
    "            x = tf.reshape(x, shape=(x.shape[0], x.shape[1], -1))\n",
    "\n",
    "        return x\n",
    "        \n",
    "    \n",
    "    def call(self, queries, keys, values, mask=None):\n",
    "        # Split queries, keys, values into multihead\n",
    "        queries = self.reshape_tensor(self.W_q(queries), self.heads, True)\n",
    "        # Output shape: (batch_size, heads, seq_len, -1)\n",
    "\n",
    "        keys = self.reshape_tensor(self.W_k(keys), self.heads, True)\n",
    "        # Output shape: (batch_size, heads, seq_len, -1)\n",
    "\n",
    "        values = self.reshape_tensor(self.W_v(values), self.heads, True)\n",
    "        # Output shape: (batch_size, heads, seq_len, -1)\n",
    "\n",
    "        output = self.attention(queries, keys, values, d_k=self.d_k, mask=mask)\n",
    "        # Output shape: (batch_size, heads, seq_len, -1)\n",
    "\n",
    "        # Concat output heads\n",
    "        output = self.reshape_tensor(output, self.heads, False)\n",
    "        \n",
    "        return self.W_o(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNormalization(layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.layer_norm = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, x_1, x_2):\n",
    "        return self.layer_norm(x_1 + x_2)\n",
    "    \n",
    "\n",
    "class FeedForward(layers.Layer):\n",
    "    \"\"\"\n",
    "    Feed forward layer with 2 connected NN layers\n",
    "    \"\"\"\n",
    "    def __init__(self, d_ff, d_model, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.W_1 = layers.Dense(d_ff, activation='relu')\n",
    "        self.W_2 = layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.W_1(x)\n",
    "\n",
    "        return self.W_2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(layers.Layer):\n",
    "    def __init__(self, heads, d_k, d_v, d_ff, d_model, rate, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.multi_head_attention = MultiHeadAttention(heads, d_k, d_v, d_model)\n",
    "        self.dropout_1 = layers.Dropout(rate)\n",
    "        self.add_norm_1 = AddNormalization()\n",
    "        self.feed_forward = FeedForward(d_ff, d_model)\n",
    "        self.dropout_2 = layers.Dropout(rate)\n",
    "        self.add_norm_2 = AddNormalization()\n",
    "\n",
    "    def call(self, x, padding_mask, training):\n",
    "        multihead_output = self.multi_head_attention(x, x, x, mask=padding_mask)\n",
    "        # Output shape: (batch_size, seq_len, d_model)\n",
    "        multihead_output = self.dropout_1(multihead_output, training=training)\n",
    "        multihead_output = self.add_norm_1(x, multihead_output)\n",
    "\n",
    "        feed_forward_output = self.feed_forward(multihead_output)\n",
    "        feed_forward_output = self.dropout_2(feed_forward_output, training=training)\n",
    "        # Output shape: (batch_size, seq_len, d_model)\n",
    "        feed_forward_output = self.add_norm_2(multihead_output, feed_forward_output)\n",
    "\n",
    "        return feed_forward_output\n",
    "\n",
    "# Possible error:\n",
    "# Only input tensors may be passed as positional arguments.\n",
    "# The following argument value should be passed as a keyword argument\n",
    "class Encoder(layers.Layer):\n",
    "    def __init__(self, seq_len, heads, d_k, d_v, d_ff, d_model, rate, N=6, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.spatial_embedding = SpatialEmbedding(seq_len, d_model)\n",
    "        self.encoder_layers = [EncoderLayer(heads, d_k, d_v, d_ff, d_model, rate) for _ in range(N)]\n",
    "\n",
    "    def call(self, x, padding_mask, training):\n",
    "        x = self.spatial_embedding(x)\n",
    "\n",
    "        for i, layer in enumerate(self.encoder_layers):\n",
    "            x = layer(x, padding_mask=padding_mask, training=training)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(layers.Layer):\n",
    "    def __init__(self, heads, d_k, d_v, d_ff, d_model, rate, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # Masked multi head attention\n",
    "        self.multi_head_attention_1 = MultiHeadAttention(heads, d_k, d_v, d_model)\n",
    "        self.dropout_1 = layers.Dropout(rate)\n",
    "        self.add_norm_1 = AddNormalization()\n",
    "\n",
    "        self.multi_head_attention_2 = MultiHeadAttention(heads, d_k, d_v, d_model)\n",
    "        self.dropout_2 = layers.Dropout(rate)\n",
    "        self.add_norm_2 = AddNormalization()\n",
    "\n",
    "        self.feed_forward = FeedForward(d_ff, d_model)\n",
    "        self.dropout_3 = layers.Dropout(rate)\n",
    "        self.add_norm_3 = AddNormalization()\n",
    "\n",
    "    def call(self, x, encoder_output, lookahead_mask, padding_mask, training):\n",
    "        multihead_output_1 = self.multi_head_attention_1(x, x, x, mask=lookahead_mask)\n",
    "        # Output shape: (batch_size, seq_len, d_model)\n",
    "        multihead_output_1 = self.dropout_1(multihead_output_1, training=training)\n",
    "        multihead_output_1 = self.add_norm_1(multihead_output_1, x)\n",
    "        # Output shape: (batch_size, seq_len, d_model)\n",
    "\n",
    "        multihead_output_2 = self.multi_head_attention_2(multihead_output_1, encoder_output, encoder_output, mask=padding_mask)\n",
    "        # Output shape: (batch_size, seq_len, d_model)\n",
    "        multihead_output_2 = self.dropout_2(multihead_output_2)\n",
    "        multihead_output_2 = self.add_norm_2(multihead_output_2, multihead_output_1)\n",
    "        # Output shape: (batch_size, seq_len, d_model)\n",
    "\n",
    "        feed_forward_output = self.feed_forward(multihead_output_2)\n",
    "        feed_forward_output = self.dropout_3(feed_forward_output)\n",
    "        feed_forward_output = self.add_norm_3(feed_forward_output, multihead_output_2)\n",
    "        # Output shape: (batch_size, seq_len, d_model)\n",
    "\n",
    "        return feed_forward_output\n",
    "    \n",
    "\n",
    "class Decoder(layers.Layer):\n",
    "    def __init__(self, vocab_size, seq_len, heads, d_k, d_v, d_ff, d_model, rate, N=6, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.word_embedding = WordEmbedding(seq_len, vocab_size, d_model)\n",
    "        self.decoder_layers = [DecoderLayer(heads, d_k, d_v, d_ff, d_model, rate) for _ in range(N)]\n",
    "\n",
    "    def call(self, output_target, encoder_output, lookahead_mask, padding_mask, training):\n",
    "        x = self.word_embedding(output_target)\n",
    "\n",
    "        for i, layer in enumerate(self.decoder_layers):\n",
    "            x = layer(x, encoder_output, lookahead_mask=lookahead_mask, padding_mask=padding_mask, training=training)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(models.Model):\n",
    "    def __init__ (self, encoder_seq_len, vocab_size, decoder_seq_len, heads, d_k, d_v, d_ff, d_model, rate, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.encoder = Encoder(encoder_seq_len, heads, d_k, d_v, d_ff, d_model, rate)\n",
    "        self.decoder = Decoder(vocab_size, decoder_seq_len, heads, d_k, d_v, d_ff, d_model, rate)\n",
    "\n",
    "        self.linear = layers.Dense(vocab_size, activation='softmax')\n",
    "\n",
    "    def encoder_padding_mask(self, input):\n",
    "        mask = tf.math.equal(input, 0)\n",
    "        \n",
    "        # Mask shape: (batch_size, seq_len, height, width, channels)\n",
    "        mask = tf.reshape(mask, (mask.shape[0], mask.shape[1], -1))\n",
    "        mask = tf.reduce_all(mask, -1)\n",
    "        mask = tf.expand_dims(mask, axis=1)\n",
    "        mask = tf.cast(mask, tf.float32)\n",
    "        # Output shape (batch_size, 1, seq_len)\n",
    "        return mask\n",
    "\n",
    "    def lookahead_mask(self, shape):\n",
    "        mask = 1 - tf.linalg.band_part(np.ones((shape, shape)), -1, 0)\n",
    "        return mask\n",
    "    \n",
    "    def call(self, encoder_input, decoder_input, training):\n",
    "        encoder_padding_mask = self.encoder_padding_mask(encoder_input)\n",
    "        decoder_lookahead_mask = self.lookahead_mask(decoder_input.shape[-1])\n",
    "\n",
    "        # For encoder, the dot-product shape is (batch_size, heads, encoder_seq_len, -1)\n",
    "        # -> the mask shape should be (batch_size, 1, encoder_seq_len, 1)\n",
    "        encoder_output = self.encoder(encoder_input, tf.expand_dims(encoder_padding_mask, axis=-1), training=training)\n",
    "\n",
    "        # For decoder, the dot-product shape is (batch_size, heads, decoder_seq_len, encoder_seq_len)\n",
    "        # -> the mask shape should be (batch_size, 1, 1, encoder_seq_len)\n",
    "        decoder_output = self.decoder(decoder_input, encoder_output, decoder_lookahead_mask, tf.expand_dims(encoder_padding_mask, axis=1), training=training)\n",
    "\n",
    "        model_output = self.linear(decoder_output)\n",
    "\n",
    "        return model_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fcn(target, prediction):\n",
    "    mask = tf.math.not_equal(target, 0)\n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "    \n",
    "    loss = sparse_categorical_crossentropy(target, prediction) * mask\n",
    "\n",
    "    return tf.reduce_sum(loss) / tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_fcn(target, prediction):\n",
    "    mask = tf.math.not_equal(target, 0)\n",
    "\n",
    "    accuracy = tf.equal(target, tf.argmax(prediction, axis=-1))\n",
    "    accuracy = tf.logical_and(accuracy, mask)\n",
    "\n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "    accuracy = tf.cast(accuracy, tf.float32)\n",
    "\n",
    "    return tf.reduce_sum(accuracy) / tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(encoder_input, decoder_input, decoder_output):\n",
    "    with GradientTape() as tape:\n",
    "        prediction = training_model(encoder_input, decoder_input, training=True)\n",
    "\n",
    "        loss = loss_fcn(decoder_output, prediction)\n",
    "\n",
    "        accuracy = accuracy_fcn(decoder_output, prediction)\n",
    "\n",
    "    gradient = tape.gradient(loss, training_model.trainable_weights)\n",
    "\n",
    "    optimizer.apply_gradient(zip(gradient, training_model.trainable_weights))\n",
    "\n",
    "    return loss, accuracy  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    train_loss = []\n",
    "    train_accuracy = []\n",
    "\n",
    "    print(f\"\\nStart of epoch: {epoch + 1} \\n\")\n",
    "\n",
    "    start = time()\n",
    "\n",
    "    for step, (train_batchX, train_batchY) in enumerate(train_dataset):\n",
    "        decoder_output = pad_sequences(train_batchY[:, 1:], decoder_seq_len)\n",
    "        print('1')\n",
    "        loss, accuracy = train_step(train_batchX, train_batchY, decoder_output)\n",
    "        print('2')\n",
    "\n",
    "        train_loss.append(loss)\n",
    "        train_accuracy.append(accuracy)\n",
    "\n",
    "        if step % 5:\n",
    "            print(f\"\\nEpoch: {epoch + 1} - Step: {step + 1}\" + \n",
    "                  f\"\\n\\t Training loss: {tf.reduce_mean(train_loss)}\" + \n",
    "                  f\"\\n\\t Training accuracy: {tf.reduct_mean(train_accuracy)}\")\n",
    "            \n",
    "    print(f\"\\nEpoch: {epoch + 1}\" + \n",
    "            f\"\\n\\t Training loss: {tf.reduce_mean(train_loss)}\" + \n",
    "            f\"\\n\\t Training accuracy: {tf.reduct_mean(train_accuracy)}\")\n",
    "    \n",
    "    print(f\"\\nTime taken: {time() - start}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
